<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Introduction</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html">
  <h3 align="center"><i>--- FEATURE ENGINEERING IS AN ART ---</i></h3>
  <br><br>
  Steps which are involved while solving any problem in machine learning are as follows:
  <ul>
    <li>Gathering data</li>
    <li>Cleaning data</li>
    <li><b>Feature engineering</b></li>
    <li>Defining model</li>
    <li>Training, testing model and predicting the output</li>
  </ul>
  <h1 id="intro">Introduction</h1>
  <p>
     What is a feature ? Basically, all machine learning algorithms use some input data to create outputs. This input data comprise features, which 
     are usually in the form of structured columns. Algorithms require features with some specific characteristic to work properly. Here, the need 
     arises with two main goals:
  </p>
  <ul>
    <li>Preparing the proper input dataset, compatible with the machine learning algorithm requirements.</li>
    <li>Improving the performance of machine learning models.</li>
  </ul>
  <h2>How to achieve them ?</h2>
  <h3>Featurization</h3>
  <p>
      At the end of the day, all our machine learning algorithms can work only with numerical (categorical, real,…) features. So it is import to change
      other types of data into numerical vectors so that we can <b>leverage the whole power of linear algebra</b> (making the decision boundary between
       data points) <b>and statistics tools</b> with other types of data also.
  </p>
  <blockquote>
  <p>Featurization is a way to change some form of data (text data, graph data, time-series data,…) into a numerical vector.</p>
  </blockquote>
  So, with featurization we can make our input data compatible with the algorithm requirements.
  Out of the various featurization techniques available for the text data, the most commonly used techniques:
    <ul>
        <li>Bag of Words (BOW)</li>
        <li>Term Frequency - Inverse Document Frequency (TF-IDF)</li>
        <li>Average Word2Vec</li>
        <li>TF-IDF Weighted Average Word2Vec</li>
    </ul>
   For the categorical features, the most commonly used featurization techniques are the mean response time and one-hot encoding.
  <h3>Feature Engineering</h3>
  <p>
    Feature Engineering is the process of using domain knowledge to <b>extract the new variables from the raw data</b>, which makes the machine learning 
    algorithms work better. Feature Engineering is a difficult task as it is domain-specific. Feature Selection is to <b>select some useful, relevant 
    features among the features</b> we generate/find/have in our data.
  </p>
  <blockquote>
     Feature engineering is about transforming the numerical features somehow so that they make machine learning models perform well at prediction.
  </blockquote>
  So, with feature engineering we can improve the performance of machine learning models.
  <p>
     <b><i>Note :- </i></b>If we are using a linear model like Logistic Regression or Linear SVM, then we perform feature engineering to convert the
      non-linearly separable data into a linearly separable format, to make our models work better. In a nutshell, we are trying to transform our 
      features to better suit our models and hence design better overall models.
  </p>
  <h3>Difference</h3>
  <p>
    Featurization is different from feature engineering. In feature engineering, features are already in the numerical form; whereas in featurization data 
    need not be in the form of numerical vector.
  </p><br><br><br><br>
  <blockquote>
    <p><b><i>
    "The features you use influence more than everything else the result. No algorithm alone, to my knowledge, can supplement the information gain given
    by correct feature engineering. Featurization and Feature Engineering is the most important aspect of ML."
    </i></b></p>
  </blockquote>
  </div>
</body>
</html>
