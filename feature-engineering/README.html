<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Introduction</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html">
  <br><br><br><br><br>
  <h2 id="intro">Introduction</h2>
  <p>
     What is a feature ? Basically, all machine learning algorithms use some input data to create outputs. This input data comprise features, which 
     are usually in the form of structured columns. Algorithms require features with some specific characteristic to work properly. Here, the need 
     arises with two main goals:
  </p>
  <ul>
    <li>Preparing the proper input dataset, compatible with the machine learning algorithm requirements.</li>
    <li>Improving the performance of machine learning models.</li>
  </ul>
  <h3>How to achieve them ?</h3>
  <p>
      At the end of the day, all our machine learning algorithms can work only with numerical (categorical, real,…) features. So it is import to change other 
      types of data into numerical vectors so that we can <b>leverage the whole power of linear algebra</b> (making the decision boundary between data points)
      <b>and statistics tools</b> with other types of data also.
  </p>
  <blockquote>
  <p>Featurization is a way to change some form of data (text data, graph data, time-series data,…) into a numerical vector.</p>
  </blockquote>
  <p>Featurization and Feature Engineering is the most important aspect of ML.</p>
  <p>
    Featurization is different from feature engineering. Feature engineering is about transforming the numerical features somehow so that they make machine
    learning models perform well at prediction. In feature engineering, features are already in the numerical form; whereas in featurization data is not need
    to be in the form of numerical vector.
  </p>
    <h2>Why Featurization?</h2>
    
    Out of the various featurization techniques available for the text data, the most commonly used techniques are :
    <ul>
        <li>Bag of Words (BOW)</li>
        <li>Term Frequency - Inverse Document Frequency (TF-IDF)</li>
        <li>Average Word2Vec</li>
        <li>TF-IDF Weighted Average Word2Vec</li>
    </ul>
   For the categorical features, the most commonly used featurization techniques are the mean response time and one-hot encoding.
   <p>
      Feature Engineering is the process of using domain knowledge to <b>extract the new variables from the raw data</b>, which makes the machine learning 
      algorithms work better. Feature Engineering is a difficult task as it is domain-specific. Feature Selection is to <b>select some useful, relevant 
      features among the features</b> we generate/find/have in our data.
   </p>
   <blockquote>
   <p>
     <b>Note :- </b>If we are using a linear model like Logistic Regression or Linear SVM, then we perform feature engineering to convert the non-linearly 
     separable data into a linearly separable format, to make our models work better. In a nutshell, we are trying to transform our features to better
     suit our models and hence design better overall models.
  </p>
  </blockquote>
  </div>
</body>
</html>

